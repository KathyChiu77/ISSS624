---
title: "Take-home Exercise 2"
author: "QIU RUILIU"
date: "6 Dec 2023"
date-modified: "last-modified"
format: html
execute: 
  eval: true
  echo: true
  warning: false
editor: visual
---

## **Setting the Scene**

The inquiry focuses on the key motivators prompting city residents to rise early for their daily commutes from home to work, and the consequences of discontinuing public bus services along specific routes. These issues represent significant challenges for transport operators and urban planners.

Traditionally, understanding these dynamics involved conducting extensive commuter surveys. These surveys, however, are expensive, time-intensive, and laborious. Moreover, the data collected often requires extensive processing and analysis, leading to reports that are frequently outdated by the time they are completed.

With the digitalization of urban infrastructure, including public buses, mass rapid transit systems, public utilities, and roads, new opportunities for data collection arise. The integration of pervasive computing technologies like GPS in vehicles and SMART cards among public transport users allows for detailed tracking of movement patterns across time and space.

Despite this, the rapid accumulation of geospatial data has overwhelmed planners' capacity to effectively analyze and convert it into valuable insights. This inefficiency negatively impacts the return on investment in data collection and management.

## **Motivation and Objective**

The purpose of this take-home project is twofold. First, it addresses the gap in applied research demonstrating the integration, analysis, and modeling of the increasingly available open data for effective policy-making. Despite the abundance of such data, there is a noticeable absence of practical studies showcasing its potential use in policy decisions.

Second, the project aims to fill the void in practical research illustrating the application of geospatial data science and analysis (GDSA) in decision-making processes.

Therefore, the assignment involves conducting a case study to showcase the value of GDSA. This will involve synthesizing publicly accessible data from various sources to construct spatial interaction models. These models will be used to identify and analyze factors influencing the urban mobility patterns of public bus transit.

## 1.Getting Started

The purpose of **`pacman`** is to streamline package management in R. One of its key functions, `p_load`, is used to install (if not already installed) and then load the specified R packages.

Inside the `p_load` function, several packages are listed:

-   **`tmap`**: Used for creating thematic maps, which are useful in geospatial analysis.

-   **`sf`**: Used for handling and analyzing spatial data.

-   **`dplyr`**: Part of the tidyverse, this package provides functions for data manipulation and transformation.

-   **`DT`**: Used for interactive display of data tables in R.

-   **`stplanr`**: A package tailored for sustainable transport planning, including functions for analyzing spatial lines, networks, etc.

-   **`performance`**: Useful for assessing and checking the performance of statistical models.

-   **`mapview`**: Facilitates interactive viewing of spatial data in R.

-   **`ggpubr`**: Provides functions to create "ggplot2"-based publication ready plots.

-   **`tidyverse`**: A collection of R packages designed for data science, including data manipulation, exploration, and visualization.

```{r}
pacman::p_load(tmap, sf, dplyr, DT,
               stplanr, performance, mapview,
               ggpubr, tidyverse, httr)
```

## 2.Data Importing

### 2.1Geospatial Data Importing

```{r}
busstop <- st_read(dsn = "data/geospatial",
                   layer = "BusStop") %>%
  st_transform(crs = 3414)
```

```{r}
mpsz <- st_read(dsn = "data/geospatial",
                   layer = "MPSZ-2019") %>%
  st_transform(crs = 3414)
```

```{r}
mpsz
```

```{r}
mpsz <- write_rds(mpsz, "data/rds/mpsz.rds")
```

### 2.2Aspatial Data Importing

```{r}
odbus <- read_csv("data/aspatial/origin_destination_bus_202310.csv")
```

```{r}
glimpse(odbus)
```

```{r}
odbus$ORIGIN_PT_CODE <- as.factor(odbus$ORIGIN_PT_CODE)
odbus$DESTINATION_PT_CODE <- as.factor(odbus$DESTINATION_PT_CODE) 
```

### **2.3Extracting the Study Data**

```{r}
weekday6_9 <- odbus %>%
  filter(DAY_TYPE == "WEEKDAY") %>%
  filter(TIME_PER_HOUR >= 6 &
           TIME_PER_HOUR <= 9) %>%
  group_by(ORIGIN_PT_CODE,
           DESTINATION_PT_CODE) %>%
  summarise(TRIPS = sum(TOTAL_TRIPS))
```

```{r}
datatable(weekday6_9)
```

We will save the output in rds format for future used.

```{r}
write_rds(weekday6_9, "data/rds/weekday6_9.rds")
```

The code chunk below will be used to import the save weekday6_9.rds into R environment.

```{r}
weekday6_9 <- read_rds("data/rds/weekday6_9.rds")
```

```{r}
weekday17_20 <- odbus %>%
  filter(DAY_TYPE == "WEEKDAY") %>%
  filter(TIME_PER_HOUR >= 17 &
           TIME_PER_HOUR <= 20) %>%
  group_by(ORIGIN_PT_CODE,
           DESTINATION_PT_CODE) %>%
  summarise(TRIPS = sum(TOTAL_TRIPS))
```

```{r}
datatable(weekday17_20)
```

```{r}
write_rds(weekday17_20, "data/rds/weekday17_20.rds")
```

```{r}
weekday17_20 <- read_rds("data/rds/weekday17_20.rds")
```

```{r}
weekend11_14 <- odbus %>%
  filter(DAY_TYPE == "WEEKENDS/HOLIDAY") %>%
  filter(TIME_PER_HOUR >= 11 &
           TIME_PER_HOUR <= 14) %>%
  group_by(ORIGIN_PT_CODE,
           DESTINATION_PT_CODE) %>%
  summarise(TRIPS = sum(TOTAL_TRIPS))
```

```{r}
datatable(weekend11_14)
```

```{r}
write_rds(weekend11_14, "data/rds/weekend11_14.rds")
```

```{r}
weekend11_14 <- read_rds("data/rds/weekend11_14.rds")
```

```{r}
weekend16_19 <- odbus %>%
  filter(DAY_TYPE == "WEEKENDS/HOLIDAY") %>%
  filter(TIME_PER_HOUR >= 16 &
           TIME_PER_HOUR <= 19) %>%
  group_by(ORIGIN_PT_CODE,
           DESTINATION_PT_CODE) %>%
  summarise(TRIPS = sum(TOTAL_TRIPS))
```

```{r}
datatable(weekend16_19)
```

```{r}
write_rds(weekend16_19, "data/rds/weekend16_19.rds")
```

```{r}
weekend16_19 <- read_rds("data/rds/weekend16_19.rds")
```

## **4.Geospatial Data Wrangling**

### **4.1Combining busstop and mpsz**

```{r}
busstop_mpsz <- st_intersection(busstop, mpsz) %>%
  select(BUS_STOP_N, SUBZONE_C)
```

```{r}
mapview(busstop_mpsz)
```

### 4.2Creating Hexagon Layer

Create hexagonal grid based on the extent of mpsz dataset

```{r}
hex_grid <- st_make_grid(mpsz, cellsize = 2 * 375 / sqrt(3), square = FALSE)
```

Convert the grid to an sf object and add an ID for each hexagon

```{r}
hex_grid_sf <- st_sf(geometry = hex_grid) %>%
  mutate(hex_id = 1:length(hex_grid))
```

Count the number of bus stops in each hexagon

```{r}
busstop_counts <- st_intersects(hex_grid_sf, busstop_mpsz, sparse = FALSE) %>% 
  apply(1, function(x) sum(x, na.rm = TRUE))
```

Add bus stop count to hex grid sf object

```{r}
hex_grid_sf$bus_stop_count <- busstop_counts
```

Filter out hexagons with no bus stops

```{r}
hex_grid_sf <- hex_grid_sf %>%
  filter(bus_stop_count > 0)
```

```{r}
tmap_options(check.and.fix = TRUE)
map_hexagon <- tm_shape(hex_grid_sf) +
  tm_polygons(
    col = "bus_stop_count",
    palette = "Purples",
    style = "cont",
    title = "Number of Bus Stops",
    id = "hex_id",
    showNA = FALSE,
    alpha = 0.6,
    popup.vars = c("Number of Bus Stops" = "bus_stop_count"),
    popup.format = list(bus_stop_count = list(format = "f", digits = 0))
  ) +
  tm_shape(mpsz) + 
  tm_borders(col = "grey75", lwd = 0.7) +
  tm_layout(
    main.title = "Bus Stop Distribution in Singapore",
    main.title.size = 1.5,
    legend.title.size = 1,
    legend.text.size = 0.8,
    legend.position = c("left", "bottom"),
    frame = FALSE,
    inner.margins = c(0.05, 0.05, 0.05, 0.05)
  ) +
  tm_credits("Data Source: LTA DataMall, Data.gov.sg", position = c("RIGHT", "BOTTOM"), size = 0.8) +
  tm_view(view.legend.position = c("left", "bottom"))

map_hexagon
```

Next, we are going to append the planning subzone code from busstop_mpsz data frame onto odbus6_9 data frame.

```{r}
busstop_hex <- st_intersection(busstop, hex_grid_sf) %>%
  select(BUS_STOP_N, hex_id) %>%
  st_drop_geometry()
```

```{r}
busstop_hex <- na.omit(busstop_hex)
```

```{r}
head(busstop_hex)
```

```{r}
write_rds(busstop_hex, "data/rds/busstop_hex.rds")  
```

```{r}
rm(hex_grid, map_hexagon, odbus, busstop_counts)
```

## 5.Preparing Commute Flow Data

### 5.1Weekday Morning Peak

```{r}
weekday_morning_od <- left_join(weekday6_9 , busstop_hex,
            by = c("ORIGIN_PT_CODE" = "BUS_STOP_N")) %>%
  rename(ORIGIN_BS = ORIGIN_PT_CODE,
         ORIGIN_HEX = hex_id,
         DESTIN_BS = DESTINATION_PT_CODE)
```

Before continue, it is a good practice for us to check for duplicating records.

```{r}
duplicate <- weekday_morning_od %>%
  group_by_all() %>%
  filter(n()>1) %>%
  ungroup()
```

If duplicated records are found, the code chunk below will be used to retain the unique records.

```{r}
weekday_morning_od <- unique(weekday_morning_od)
```

```{r}
weekday_morning_od <- left_join(weekday_morning_od , busstop_hex,
            by = c("DESTIN_BS" = "BUS_STOP_N"))
```

```{r}
duplicate <- weekday_morning_od %>%
  group_by_all() %>%
  filter(n()>1) %>%
  ungroup()
```

```{r}
weekday_morning_od <- unique(weekday_morning_od)
```

```{r}
weekday_morning_od <- weekday_morning_od %>%
  rename(DESTIN_HEX = hex_id) %>%
  drop_na() %>%
  group_by(ORIGIN_HEX, DESTIN_HEX) %>%
  summarise(WEEKDAY_MORNING_PEAK = sum(TRIPS))
```

It is time to save the output into an rds file format.

```{r}
write_rds(weekday_morning_od, "data/rds/weekday_morning_od.rds")
```

```{r}
weekday_morning_od <- read_rds("data/rds/weekday_morning_od.rds")
```

### 5.2Weekday Afternoon Peak

```{r}
weekday_afternoon_od <- left_join(weekday17_20 , busstop_hex,
            by = c("ORIGIN_PT_CODE" = "BUS_STOP_N")) %>%
  rename(ORIGIN_BS = ORIGIN_PT_CODE,
         ORIGIN_HEX = hex_id,
         DESTIN_BS = DESTINATION_PT_CODE)
```

```{r}
duplicate <- weekday_afternoon_od %>%
  group_by_all() %>%
  filter(n()>1) %>%
  ungroup()
```

```{r}
weekday_afternoon_od <- unique(weekday_afternoon_od)
```

```{r}
weekday_afternoon_od <- left_join(weekday_afternoon_od , busstop_hex,
            by = c("DESTIN_BS" = "BUS_STOP_N"))
```

```{r}
duplicate <- weekday_afternoon_od %>%
  group_by_all() %>%
  filter(n()>1) %>%
  ungroup()
```

```{r}
weekday_afternoon_od <- unique(weekday_afternoon_od)
```

```{r}
weekday_afternoon_od <- weekday_afternoon_od %>%
  rename(DESTIN_HEX = hex_id) %>%
  drop_na() %>%
  group_by(ORIGIN_HEX, DESTIN_HEX) %>%
  summarise(WEEKDAY_AFTERNOON_PEAK = sum(TRIPS))
```

```{r}
write_rds(weekday_afternoon_od, "data/rds/weekday_afternoon_od.rds")
```

```{r}
weekday_afternoon_od <- read_rds("data/rds/weekday_afternoon_od.rds")
```

### 5.3Weekend Morning Peak

```{r}
weekend_morning_od <- left_join(weekend11_14 , busstop_hex,
            by = c("ORIGIN_PT_CODE" = "BUS_STOP_N")) %>%
  rename(ORIGIN_BS = ORIGIN_PT_CODE,
         ORIGIN_HEX = hex_id,
         DESTIN_BS = DESTINATION_PT_CODE)
```

```{r}
duplicate <- weekend_morning_od %>%
  group_by_all() %>%
  filter(n()>1) %>%
  ungroup()
```

```{r}
weekend_morning_od <- unique(weekend_morning_od)
```

```{r}
weekend_morning_od <- left_join(weekend_morning_od , busstop_hex,
            by = c("DESTIN_BS" = "BUS_STOP_N"))
```

```{r}
duplicate <- weekend_morning_od %>%
  group_by_all() %>%
  filter(n()>1) %>%
  ungroup()
```

```{r}
weekend_morning_od <- unique(weekend_morning_od)
```

```{r}
weekend_morning_od <- weekend_morning_od %>%
  rename(DESTIN_HEX = hex_id) %>%
  drop_na() %>%
  group_by(ORIGIN_HEX, DESTIN_HEX) %>%
  summarise(WEEKEND_MORNING_PEAK = sum(TRIPS))
```

```{r}
write_rds(weekend_morning_od, "data/rds/weekend_morning_od.rds")
```

```{r}
weekend_morning_od <- read_rds("data/rds/weekend_morning_od.rds")
```

### 5.4Weekend Evening Peak

```{r}
weekend_evening_od <- left_join(weekend16_19 , busstop_hex,
            by = c("ORIGIN_PT_CODE" = "BUS_STOP_N")) %>%
  rename(ORIGIN_BS = ORIGIN_PT_CODE,
         ORIGIN_HEX = hex_id,
         DESTIN_BS = DESTINATION_PT_CODE)
```

```{r}
duplicate <- weekend_evening_od %>%
  group_by_all() %>%
  filter(n()>1) %>%
  ungroup()
```

```{r}
weekend_evening_od <- unique(weekend_evening_od)
```

```{r}
weekend_evening_od <- left_join(weekend_evening_od , busstop_hex,
            by = c("DESTIN_BS" = "BUS_STOP_N"))
```

```{r}
duplicate <- weekend_evening_od %>%
  group_by_all() %>%
  filter(n()>1) %>%
  ungroup()
```

```{r}
weekend_evening_od <- unique(weekend_evening_od)
```

```{r}
weekend_evening_od <- weekend_evening_od %>%
  rename(DESTIN_HEX = hex_id) %>%
  drop_na() %>%
  group_by(ORIGIN_HEX, DESTIN_HEX) %>%
  summarise(WEEKEND_EVENING_PEAK = sum(TRIPS))
```

```{r}
write_rds(weekend_evening_od, "data/rds/weekend_evening_od.rds")
```

```{r}
weekend_evening_od <- read_rds("data/rds/weekend_evening_od.rds")
```

```{r}
rm(weekday6_9, weekday17_20, weekend11_14, weekend16_19)
```

## **6.Visualising Spatial Interaction**

### **6.1Removing Intra-zonal Flows**

```{r}
weekday_morning_od1 <- weekday_morning_od[weekday_morning_od$ORIGIN_HEX!=weekday_morning_od$DESTIN_HEX,]
```

```{r}
weekday_afternoon_od1 <- weekday_afternoon_od[weekday_afternoon_od$ORIGIN_HEX!=weekday_afternoon_od$DESTIN_HEX,]
```

```{r}
weekend_morning_od1 <- weekend_morning_od[weekend_morning_od$ORIGIN_HEX!=weekend_morning_od$DESTIN_HEX,]
```

```{r}
weekend_evening_od1 <- weekend_evening_od[weekend_evening_od$ORIGIN_HEX!=weekend_evening_od$DESTIN_HEX,]
```

### 6.2Creating the Desire Lines

```{r}
weekday_morning_flowLine <- od2line(flow = weekday_morning_od1, 
                    zones = hex_grid_sf,
                    zone_code = "hex_id")
```

```{r}
weekday_afternoon_flowLine <- od2line(flow = weekday_afternoon_od1, 
                    zones = hex_grid_sf,
                    zone_code = "hex_id")
```

```{r}
weekend_morning_flowLine <- od2line(flow = weekend_morning_od1, 
                    zones = hex_grid_sf,
                    zone_code = "hex_id")
```

```{r}
weekend_evening_flowLine <- od2line(flow = weekend_evening_od1, 
                    zones = hex_grid_sf,
                    zone_code = "hex_id")
```

### 6.3Visualising the Desire Lines

```{r}
tm_shape(hex_grid_sf) +
  tm_polygons(
    border.col = "grey50", 
    border.alpha = 0.6, 
    alpha = 0.1
  ) +
  tm_shape(weekday_morning_flowLine %>% 
             filter(WEEKDAY_MORNING_PEAK >= 5000)) +
  tm_lines(
    lwd = "WEEKDAY_MORNING_PEAK",
    style = "quantile",
    scale = c(0.1, 1, 3, 5, 7, 10),
    n = 6,
    alpha = 0.5,
    palette = "Blues"
  ) +
  tm_shape(mpsz) +
  tm_borders(
    col = "darkblue", 
    alpha = 0.1,
    lwd = 1.5
  ) +
  tm_layout(
    main.title = "Weekday Morning Commute Flows in Singapore",
    main.title.position = "center",
    main.title.size = 1.0,
    legend.title.size = 0.8,
    legend.text.size = 0.7,
    legend.position = c("left", "bottom"),
    frame = FALSE,
    inner.margins = c(0.05, 0.05, 0.05, 0.05)
  ) +
  tm_credits("Source: LTA DataMall", position = c("RIGHT", "BOTTOM"), size = 0.5)
```

```{r}
tm_shape(hex_grid_sf) +
  tm_polygons(
    border.col = "grey50", 
    border.alpha = 0.6, 
    alpha = 0.1
  ) +
  tm_shape(weekday_afternoon_flowLine %>% 
             filter(WEEKDAY_AFTERNOON_PEAK >= 5000)) +
  tm_lines(
    lwd = "WEEKDAY_AFTERNOON_PEAK",
    style = "quantile",
    scale = c(0.1, 1, 3, 5, 7, 10),
    n = 6,
    alpha = 0.5,
    palette = "Blues"
  ) +
  tm_shape(mpsz) +
  tm_borders(
    col = "darkblue", 
    alpha = 0.1,
    lwd = 1.5
  ) +
  tm_layout(
    main.title = "Weekday Afternoon Commute Flows in Singapore",
    main.title.position = "center",
    main.title.size = 1.0,
    legend.title.size = 0.8,
    legend.text.size = 0.7,
    legend.position = c("left", "bottom"),
    frame = FALSE,
    inner.margins = c(0.05, 0.05, 0.05, 0.05)
  ) +
  tm_credits("Source: LTA DataMall", position = c("RIGHT", "BOTTOM"), size = 0.5)
```

```{r}
tm_shape(hex_grid_sf) +
  tm_polygons(
    border.col = "grey50", 
    border.alpha = 0.6, 
    alpha = 0.1
  ) +
  tm_shape(weekend_morning_flowLine %>% 
             filter(WEEKEND_MORNING_PEAK >= 3000)) +
  tm_lines(
    lwd = "WEEKEND_MORNING_PEAK",
    style = "quantile",
    scale = c(0.1, 1, 3, 5, 7, 13, 15),
    n = 7,
    alpha = 0.5,
    palette = "Blues"
  ) +
  tm_shape(mpsz) +
  tm_borders(
    col = "darkblue", 
    alpha = 0.1,
    lwd = 1.5
  ) +
  tm_layout(
    main.title = "Weekend Morning Commute Flows in Singapore",
    main.title.position = "center",
    main.title.size = 1.0,
    legend.title.size = 0.8,
    legend.text.size = 0.7,
    legend.position = c("left", "bottom"),
    frame = FALSE,
    inner.margins = c(0.05, 0.05, 0.05, 0.05)
  ) +
  tm_credits("Source: LTA DataMall", position = c("RIGHT", "BOTTOM"), size = 0.5)
```

```{r}
tm_shape(hex_grid_sf) +
  tm_polygons(
    border.col = "grey50", 
    border.alpha = 0.6, 
    alpha = 0.1
  ) +
  tm_shape(weekend_evening_flowLine %>% 
             filter(WEEKEND_EVENING_PEAK >= 3000)) +
  tm_lines(
    lwd = "WEEKEND_EVENING_PEAK",
    style = "quantile",
    scale = c(0.1, 1, 3, 5, 7, 10),
    n = 6,
    alpha = 0.5,
    palette = "Blues"
  ) +
  tm_shape(mpsz) +
  tm_borders(
    col = "darkblue", 
    alpha = 0.1,
    lwd = 1.5
  ) +
  tm_layout(
    main.title = "Weekend Evening Commute Flows in Singapore",
    main.title.position = "center",
    main.title.size = 1.0,
    legend.title.size = 0.8,
    legend.text.size = 0.7,
    legend.position = c("left", "bottom"),
    frame = FALSE,
    inner.margins = c(0.05, 0.05, 0.05, 0.05)
  ) +
  tm_credits("Source: LTA DataMall", position = c("RIGHT", "BOTTOM"), size = 0.5)
```

```{r}
rm(duplicate, weekday_morning_flowLine, weekday_morning_od, weekday_morning_od1,
   weekend_morning_flowLine, weekend_morning_od, weekend_morning_od1,
   weekend_evening_flowLine, weekend_evening_od, weekend_evening_od1)
```

## 7.Attractiveness Factors

### 7.1Data Integration

```{r}
entertn <- st_read(dsn = "data/geospatial",
                      layer = "entertn")
```

```{r}
hex_grid_sf$entertn_count <- lengths(st_intersects(hex_grid_sf, entertn))
```

```{r}
FB <- st_read(dsn = "data/geospatial",
                   layer = "F&B")
```

```{r}
hex_grid_sf$FB_count <- lengths(st_intersects(hex_grid_sf, FB))
```

```{r}
lere <- st_read(dsn = "data/geospatial",
                   layer = "Liesure&Recreation")
```

```{r}
hex_grid_sf$lere_count <- lengths(st_intersects(hex_grid_sf, lere))
```

```{r}
retail <- st_read(dsn = "data/geospatial",
                  layer = "Retails")
```

```{r}
hex_grid_sf$retail_count <- lengths(st_intersects(hex_grid_sf, retail))
```

```{r}
trainexits <- st_read(dsn = "data/geospatial",
                      layer = "Train_Station_Exit_Layer")
```

```{r}
trainexits <- st_transform(trainexits, st_crs(hex_grid_sf))
hex_grid_sf$trainexits_count <- lengths(st_intersects(hex_grid_sf, trainexits))
```

```{r}
hdb <- read_csv("data/aspatial/hdb.csv")
```

```{r}
residential <- hdb %>%
  filter(residential == "Y") %>%
  select(lat, lng, total_dwelling_units) %>%
  na.omit()
```

```{r}
residential <- st_as_sf(residential, 
                           coords = c("lng", "lat"), 
                           crs = 4326) %>%
  st_transform(crs = st_crs(hex_grid_sf))
```

```{r}
intersections <- st_intersects(hex_grid_sf, residential, sparse = TRUE)
hex_grid_sf$residential_count <- mapply(function(index, residential) {
  sum(residential$total_dwelling_units[index], na.rm = TRUE)
}, intersections, MoreArgs = list(residential = residential))
```

```{r}
rm(intersections)
```

## 8.Propulsive Factors

### 8.1Data Integration

```{r}
business <- st_read(dsn = "data/geospatial",
                      layer = "Business")
```

```{r}
hex_grid_sf$business_count <- lengths(st_intersects(hex_grid_sf, business))
```

```{r}
url<-"https://www.onemap.gov.sg/api/common/elastic/search"

csv<-read_csv("data/aspatial/Generalinformationofschools.csv")
postcodes<-csv$`postal_code`

found<-data.frame()
not_found<-data.frame()

for(postcode in postcodes){
  query<-list('searchVal'=postcode,'returnGeom'='Y','getAddrDetails'='Y','pageNum'='1')
  res<- GET(url,query=query)
  
  if((content(res)$found)!=0){
    found<-rbind(found,data.frame(content(res))[4:13])
  } else{
    not_found = data.frame(postcode)
  }
}
```

```{r}
merged = merge(csv, found, by.x = 'postal_code', by.y = 'results.POSTAL', all = TRUE)
write.csv(merged, file = "data/aspatial/schools.csv")
write.csv(not_found, file = "data/aspatial/not_found.csv")
```

```{r}
schools <- read_csv("data/aspatial/schools.csv") %>%
  rename(latitude = "results.LATITUDE",
         longitude = "results.LONGITUDE")%>%
  select(postal_code, school_name, latitude, longitude)
```

```{r}
schools <- schools %>%
  filter(!is.na(longitude) & !is.na(latitude)) %>%
  st_as_sf(coords = c("longitude", "latitude"), crs = 4326) %>%
  st_transform(crs = 3414)
```

```{r}
hex_grid_sf$school_count <- lengths(st_intersects(hex_grid_sf, schools))
```

```{r}
rm(csv, found, merged, not_found, query, res,
   postcode, postcodes, url)
```

```{r}
finserv <- st_read(dsn = "data/geospatial",
                      layer = "FinServ")
```

```{r}
hex_grid_sf$finserv_count <- lengths(st_intersects(hex_grid_sf, finserv))
```

### 8.2Final Check

```{r}
datatable(hex_grid_sf)
```
