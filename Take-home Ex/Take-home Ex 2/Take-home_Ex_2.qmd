---
title: "Take-home Exercise 2: Analyzing the Dynamics of Bus Commute Flow and Spatial Interaction in Singapore"
author: "QIU RUILIU"
date: "6 Dec 2023"
date-modified: "last-modified"
format: html
execute: 
  eval: true
  echo: true
  warning: false
editor: visual
---

## **Setting the Scene**

The inquiry focuses on the key motivators prompting city residents to rise early for their daily commutes from home to work, and the consequences of discontinuing public bus services along specific routes. These issues represent significant challenges for transport operators and urban planners.

Traditionally, understanding these dynamics involved conducting extensive commuter surveys. These surveys, however, are expensive, time-intensive, and laborious. Moreover, the data collected often requires extensive processing and analysis, leading to reports that are frequently outdated by the time they are completed.

With the digitalization of urban infrastructure, including public buses, mass rapid transit systems, public utilities, and roads, new opportunities for data collection arise. The integration of pervasive computing technologies like GPS in vehicles and SMART cards among public transport users allows for detailed tracking of movement patterns across time and space.

Despite this, the rapid accumulation of geospatial data has overwhelmed planners' capacity to effectively analyze and convert it into valuable insights. This inefficiency negatively impacts the return on investment in data collection and management.

## **Motivation and Objective**

The purpose of this take-home project is twofold. First, it addresses the gap in applied research demonstrating the integration, analysis, and modeling of the increasingly available open data for effective policy-making. Despite the abundance of such data, there is a noticeable absence of practical studies showcasing its potential use in policy decisions.

Second, the project aims to fill the void in practical research illustrating the application of geospatial data science and analysis (GDSA) in decision-making processes.

Therefore, the assignment involves conducting a case study to showcase the value of GDSA. This will involve synthesizing publicly accessible data from various sources to construct spatial interaction models. These models will be used to identify and analyze factors influencing the urban mobility patterns of public bus transit.

## 1.Getting Started

The code snippet shown is responsible for loading various packages that provide essential tools and functions for the analysis.

```{r}
pacman::p_load(tmap, sf, dplyr, DT, sp,
               stplanr, performance, mapview,
               ggpubr, tidyverse, httr,
               units, reshape2)
```

-   **`pacman::p_load`**: This function from the **`pacman`** package streamlines the process of loading multiple R packages. If a package is not already installed, `p_load` will install it before loading.

-   **`tmap`**: Utilized for creating thematic maps, essential in visualizing geospatial data.

-   **`sf`**: Stands for "simple features" and is used for handling and analyzing geospatial data.

-   **`dplyr`**: A part of the **`tidyverse`** collection, this package is instrumental in data manipulation tasks like filtering, selecting, and summarizing data.

-   **`DT`**: Provides an R interface to the JavaScript library "DataTables", enabling interactive display of data in tables.

-   **`sp`**: Offers classes and methods for spatial data, crucial for handling spatial points, lines, and polygons.

-   **`stplanr`**: Specifically designed for sustainable transport planning with spatial data.

-   **`performance`**: Useful for assessing and comparing the performance of statistical models.

-   **`mapview`**: Facilitates interactive viewing of spatial data in R.

-   **`ggpubr`**: A part of the **`ggplot2`** ecosystem, this package provides additional functions for creating publication-ready plots.

-   **`tidyverse`**: A collection of R packages designed for data science, providing tools for data manipulation, visualization, and more.

-   **`httr`**: Used for working with HTTP protocols to access web resources.

-   **`units`**: Deals with measurement units, crucial for handling and converting between different units of measurement in spatial data.

-   **`reshape2`**: Aids in reshaping data, transitioning between wide and long formats, which is often necessary in data analysis.

Each of these packages plays a specific role in the analysis, ranging from data manipulation and visualization to handling spatial and web-based data.

## 2.Data Importing

### 2.1Geospatial Data Importing

This R code snippet is focused on importing and transforming geospatial data related to Singapore's bus stops and Metropolitan Planning Strategy Zones (MPSZ) for the year 2019. Using `st_read` from the **`sf`** package, it loads the **BusStop** and **MPSZ-2019** layers from a specified directory (**data/geospatial**). Both datasets are then transformed to the local Singapore coordinate reference system (CRS code 3414) using `st_transform`.

```{r}
busstop <- st_read(dsn = "data/geospatial",
                   layer = "BusStop") %>%
  st_transform(crs = 3414)
```

```{r}
mpsz <- st_read(dsn = "data/geospatial",
                   layer = "MPSZ-2019") %>%
  st_transform(crs = 3414)
```

Display and grasp the basic situation of geospatial dataset **mpsz**.

```{r}
mpsz
```

Export and save in **rds** format for later use.

```{r}
mpsz <- write_rds(mpsz, "data/rds/mpsz.rds")
```

### 2.2Aspatial Data Importing

This line of R code is used for importing an aspatial dataset named **origin_destination_bus_202310.csv** from a specified directory (**data/aspatial**). The function `read_csv` from the **`tidyverse`** package efficiently reads the CSV file, converting it into a dataframe. This dataset likely contains origin-destination information for bus routes in October, 2023.

```{r}
odbus <- read_csv("data/aspatial/origin_destination_bus_202310.csv")
```

The `glimpse(odbus)` function provides a quick overview of the structure and contents of the **odbus** dataframe, summarizing its columns, data types, and a few initial entries.

```{r}
glimpse(odbus)
```

This code converts the **ORIGIN_PT_CODE** and **DESTINATION_PT_CODE** columns in the **odbus** dataframe to factors, categorizing unique bus stop codes for analysis.

```{r}
odbus$ORIGIN_PT_CODE <- as.factor(odbus$ORIGIN_PT_CODE)
odbus$DESTINATION_PT_CODE <- as.factor(odbus$DESTINATION_PT_CODE) 
```

### **2.3Extracting the Study Data**

#### 2.3.1Extracting the Trips Volume of Weekday Morning Peak

This code snippet filters and summarizes the **odbus** dataframe to extract data on bus trips during weekday morning peak hours (6 to 9 AM). It selects records marked as **WEEKDAY**, groups them by origin and destination bus stop codes, and then calculates the total number of trips between each stop pair in this time frame.

```{r}
weekday6_9 <- odbus %>%
  filter(DAY_TYPE == "WEEKDAY") %>%
  filter(TIME_PER_HOUR >= 6 &
           TIME_PER_HOUR <= 9) %>%
  group_by(ORIGIN_PT_CODE,
           DESTINATION_PT_CODE) %>%
  summarise(TRIPS = sum(TOTAL_TRIPS))
```

The `datatable(weekday6_9)` function creates an interactive table display of the **weekday6_9** dataframe, facilitating easy exploration and analysis of the data.

```{r}
datatable(weekday6_9)
```

The `write_rds(weekday6_9, "data/rds/weekday6_9.rds")` function saves the **weekday6_9** dataframe to a file named **weekday6_9.rds** for future use.

```{r eval=FALSE}
write_rds(weekday6_9, "data/rds/weekday6_9.rds")
```

The `read_rds("data/rds/weekday6_9.rds")` function loads the previously saved **weekday6_9** dataframe back into the R environment.

```{r}
weekday6_9 <- read_rds("data/rds/weekday6_9.rds")
```

#### 2.3.2Extracting the Trips Volume of Weekday Afternoon Peak

Repeat the process above for weekday afternoon peak (5-8 PM).

```{r}
weekday17_20 <- odbus %>%
  filter(DAY_TYPE == "WEEKDAY") %>%
  filter(TIME_PER_HOUR >= 17 &
           TIME_PER_HOUR <= 20) %>%
  group_by(ORIGIN_PT_CODE,
           DESTINATION_PT_CODE) %>%
  summarise(TRIPS = sum(TOTAL_TRIPS))
```

```{r}
datatable(weekday17_20)
```

```{r eval=FALSE}
write_rds(weekday17_20, "data/rds/weekday17_20.rds")
```

```{r}
weekday17_20 <- read_rds("data/rds/weekday17_20.rds")
```

#### 2.3.3Extracting the Trips Volume of Weekend/Holiday Morning Peak

Repeat the process above for weekend/holiday morning peak (11 AM-2 PM).

```{r}
weekend11_14 <- odbus %>%
  filter(DAY_TYPE == "WEEKENDS/HOLIDAY") %>%
  filter(TIME_PER_HOUR >= 11 &
           TIME_PER_HOUR <= 14) %>%
  group_by(ORIGIN_PT_CODE,
           DESTINATION_PT_CODE) %>%
  summarise(TRIPS = sum(TOTAL_TRIPS))
```

```{r}
datatable(weekend11_14)
```

```{r eval=FALSE}
write_rds(weekend11_14, "data/rds/weekend11_14.rds")
```

```{r}
weekend11_14 <- read_rds("data/rds/weekend11_14.rds")
```

#### 2.3.4Extracting the Trips Volume of Weekend/Holiday Evening Peak

Repeat the process above for weekend/holiday evening peak (4-7 PM).

```{r}
weekend16_19 <- odbus %>%
  filter(DAY_TYPE == "WEEKENDS/HOLIDAY") %>%
  filter(TIME_PER_HOUR >= 16 &
           TIME_PER_HOUR <= 19) %>%
  group_by(ORIGIN_PT_CODE,
           DESTINATION_PT_CODE) %>%
  summarise(TRIPS = sum(TOTAL_TRIPS))
```

```{r}
datatable(weekend16_19)
```

```{r eval=FALSE}
write_rds(weekend16_19, "data/rds/weekend16_19.rds")
```

```{r}
weekend16_19 <- read_rds("data/rds/weekend16_19.rds")
```

## **4.Geospatial Data Wrangling**

### **4.1Combining busstop and mpsz**

The code `st_intersection(busstop, mpsz)` combines the **busstop** and **mpsz** datasets to retain only those bus stops located within the boundaries of Singapore's metropolitan planning zones. The `select(BUS_STOP_N, SUBZONE_C)` part then extracts specific columns, namely bus stop IDs and subzone codes, from the intersected dataset.

```{r}
busstop_mpsz <- st_intersection(busstop, mpsz) %>%
  select(BUS_STOP_N, SUBZONE_C)
```

The `mapview(busstop_mpsz)` function creates an interactive map displaying the spatial data from the **busstop_mpsz** dataframe, visualizing the locations of bus stops within Singapore's planning zones.

```{r}
mapview(busstop_mpsz)
```

### 4.2Creating Hexagon Layer

The code `st_make_grid(mpsz, cellsize = 2 * 375 / sqrt(3), square = FALSE)` generates a hexagonal grid overlay on the **mpsz** spatial data. Each hexagon in the grid has a perpendicular distance from its center to its edges of 375 meters, effectively creating a hexagonal pattern to represent [Traffic Analysis Zones (TAZs)](https://tmg.utoronto.ca/files/Reports/Traffic-Zone-Guidance_March-2021_Final.pdf).

```{r}
hex_grid <- st_make_grid(mpsz, cellsize = 2 * 375 / sqrt(3), square = FALSE)
```

This code converts the **hex_grid** object into a simple features (sf) dataframe **hex_grid_sf**, and then adds a new column **hex_id** that assigns a unique identifier to each hexagon in the grid.

```{r}
hex_grid_sf <- st_sf(geometry = hex_grid) %>%
  mutate(hex_id = 1:length(hex_grid))
```

This code calculates the number of bus stops within each hexagon of the **hex_grid_sf** grid. It uses `st_intersects` to identify which bus stops (**busstop_mpsz**) fall within each hexagon, and then applies a function to count these stops for each hexagon, handling any missing values (`na.rm = TRUE`).

```{r}
busstop_counts <- st_intersects(hex_grid_sf, busstop_mpsz, sparse = FALSE) %>% 
  apply(1, function(x) sum(x, na.rm = TRUE))
```

This step assigns the computed bus stop counts (`busstop_counts`) to a new column `bus_stop_count` in the **hex_grid_sf** dataframe, effectively adding the number of bus stops within each hexagon to the grid data.

```{r}
hex_grid_sf$bus_stop_count <- busstop_counts
```

This code filters the **hex_grid_sf** dataframe to keep only those hexagons that have one or more bus stops, effectively removing hexagons with no bus stop presence.

```{r}
hex_grid_sf <- hex_grid_sf %>%
  filter(bus_stop_count > 0)
```

This code creates a visual map in R using the **`tmap`** package. It overlays hexagons from **hex_grid_sf** onto the **mpsz** spatial layout, coloring them based on the count of bus stops in each hexagon. The map features a legend, titles, and styling details for clarity and visual appeal. The final map displays the distribution of bus stops across Singapore, with additional borders for context and credits for data sources.

```{r}
tmap_options(check.and.fix = TRUE)
map_hexagon <- tm_shape(hex_grid_sf) +
  tm_polygons(
    col = "bus_stop_count",
    palette = "Purples",
    style = "cont",
    title = "Number of Bus Stops",
    id = "hex_id",
    showNA = FALSE,
    alpha = 0.6,
    popup.vars = c("Number of Bus Stops" = "bus_stop_count"),
    popup.format = list(bus_stop_count = list(format = "f", digits = 0))
  ) +
  tm_shape(mpsz) + 
  tm_borders(col = "grey75", lwd = 0.7) +
  tm_layout(
    main.title = "Bus Stop Distribution in Singapore",
    main.title.size = 1.5,
    legend.title.size = 1,
    legend.text.size = 0.8,
    legend.position = c("left", "bottom"),
    frame = FALSE,
    inner.margins = c(0.05, 0.05, 0.05, 0.05)
  ) +
  tm_credits("Data Source: LTA DataMall, Data.gov.sg", position = c("RIGHT", "BOTTOM"), size = 0.8) +
  tm_view(view.legend.position = c("left", "bottom"))

map_hexagon
```

[Observations from the map:]{.underline}

-   Blank Areas: The absence of hexagons in certain parts of the map suggests these are areas with no bus stops. These could be non-residential areas, such as industrial zones, green spaces, or water bodies where public bus services are not necessary or practical.

-   Bus Stop-Dense Areas: Regions densely packed with hexagons indicate a high concentration of bus stops. These areas are likely to be highly urbanized with significant residential and commercial activities, necessitating a greater number of bus stops to accommodate the public transport needs of the population.

The distribution pattern reflects the urban planning and public transportation infrastructure of Singapore, designed to cater to areas with high commuter demand while excluding zones where bus stops are not viable.

### 4.3Correspondence of Hexagon and Bus Stop ID

The code creates a new dataset **busstop_hex** by intersecting **busstop** locations with the **hex_grid_sf** to assign a **hex_id** to each bus stop. It then selects the bus stop number and corresponding **hex_id**, and removes the spatial geometry data for a simple reference table. The final step omits any entries with missing values to ensure a clean dataset for merging with other data based on hexagon and bus stop IDs.

```{r}
busstop_hex <- st_intersection(busstop, hex_grid_sf) %>%
  select(BUS_STOP_N, hex_id) %>%
  st_drop_geometry()
```

```{r}
busstop_hex <- na.omit(busstop_hex)
```

The `head(busstop_hex)` function displays the first few rows of the **busstop_hex** dataframe for a quick preview of its structure and data.

```{r}
head(busstop_hex)
```

Export and save **busstop_hex** in rds format for future use.

```{r}
write_rds(busstop_hex, "data/rds/busstop_hex.rds")  
```

Remove items no longer needed from R environment to free memory and avoid redundancy.

```{r}
rm(hex_grid, map_hexagon, odbus, busstop_counts)
```

## 5.Preparing Commute Flow Data

### 5.1Weekday Morning Peak Flow

The code merges flow data from **weekday6_9** with bus stop IDs from **busstop_hex** based on common origin bus stop numbers, then renames the key columns for clarity, assigning hex IDs to origin points and preserving destination bus stop codes.

```{r eval=FALSE}
weekday_morning_od <- left_join(weekday6_9 , busstop_hex,
            by = c("ORIGIN_PT_CODE" = "BUS_STOP_N")) %>%
  rename(ORIGIN_BS = ORIGIN_PT_CODE,
         ORIGIN_HEX = hex_id,
         DESTIN_BS = DESTINATION_PT_CODE)
```

Before continue, it is a good practice for us to check for duplicating records (It can be judged based on data frame is blank or not).

```{r eval=FALSE}
duplicate <- weekday_morning_od %>%
  group_by_all() %>%
  filter(n()>1) %>%
  ungroup()
```

If duplicated records are found, the code chunk below will be used to retain the unique records.

```{r eval=FALSE}
weekday_morning_od <- unique(weekday_morning_od)
```

This code further enhances the **weekday_morning_od** dataframe by joining it with the **busstop_hex** dataframe to append **hex_id** information corresponding to the destination bus stops, linking each trip's endpoint to its respective hexagonal spatial zone.

```{r eval=FALSE}
weekday_morning_od <- left_join(weekday_morning_od , busstop_hex,
            by = c("DESTIN_BS" = "BUS_STOP_N"))
```

```{r eval=FALSE}
duplicate <- weekday_morning_od %>%
  group_by_all() %>%
  filter(n()>1) %>%
  ungroup()
```

```{r eval=FALSE}
weekday_morning_od <- unique(weekday_morning_od)
```

This step renames the destination hex ID column for clarity, removes any rows with missing data, then groups the data by origin and destination hex IDs, and summarizes it to calculate the total number of trips made during the weekday morning peak hours between each hexagon pair.

```{r eval=FALSE}
weekday_morning_od <- weekday_morning_od %>%
  rename(DESTIN_HEX = hex_id) %>%
  drop_na() %>%
  group_by(ORIGIN_HEX, DESTIN_HEX) %>%
  summarise(WEEKDAY_MORNING_PEAK = sum(TRIPS))
```

It is time to save the output into an rds file format.

```{r eval=FALSE}
write_rds(weekday_morning_od, "data/rds/weekday_morning_od.rds")
```

```{r}
weekday_morning_od <- read_rds("data/rds/weekday_morning_od.rds")
```

### 5.2Weekday Afternoon Peak Flow

Repeat the process above for weekday afternoon peak (5-8 PM).

```{r eval=FALSE}
weekday_afternoon_od <- left_join(weekday17_20 , busstop_hex,
            by = c("ORIGIN_PT_CODE" = "BUS_STOP_N")) %>%
  rename(ORIGIN_BS = ORIGIN_PT_CODE,
         ORIGIN_HEX = hex_id,
         DESTIN_BS = DESTINATION_PT_CODE)
```

```{r eval=FALSE}
duplicate <- weekday_afternoon_od %>%
  group_by_all() %>%
  filter(n()>1) %>%
  ungroup()
```

```{r eval=FALSE}
weekday_afternoon_od <- unique(weekday_afternoon_od)
```

```{r eval=FALSE}
weekday_afternoon_od <- left_join(weekday_afternoon_od , busstop_hex,
            by = c("DESTIN_BS" = "BUS_STOP_N"))
```

```{r eval=FALSE}
duplicate <- weekday_afternoon_od %>%
  group_by_all() %>%
  filter(n()>1) %>%
  ungroup()
```

```{r eval=FALSE}
weekday_afternoon_od <- unique(weekday_afternoon_od)
```

```{r eval=FALSE}
weekday_afternoon_od <- weekday_afternoon_od %>%
  rename(DESTIN_HEX = hex_id) %>%
  drop_na() %>%
  group_by(ORIGIN_HEX, DESTIN_HEX) %>%
  summarise(WEEKDAY_AFTERNOON_PEAK = sum(TRIPS))
```

```{r eval=FALSE}
write_rds(weekday_afternoon_od, "data/rds/weekday_afternoon_od.rds")
```

```{r}
weekday_afternoon_od <- read_rds("data/rds/weekday_afternoon_od.rds")
```

### 5.3Weekend Morning Peak Flow

Repeat the process above for weekend/holiday morning peak (11 AM-2 PM).

```{r eval=FALSE}
weekend_morning_od <- left_join(weekend11_14 , busstop_hex,
            by = c("ORIGIN_PT_CODE" = "BUS_STOP_N")) %>%
  rename(ORIGIN_BS = ORIGIN_PT_CODE,
         ORIGIN_HEX = hex_id,
         DESTIN_BS = DESTINATION_PT_CODE)
```

```{r eval=FALSE}
duplicate <- weekend_morning_od %>%
  group_by_all() %>%
  filter(n()>1) %>%
  ungroup()
```

```{r eval=FALSE}
weekend_morning_od <- unique(weekend_morning_od)
```

```{r eval=FALSE}
weekend_morning_od <- left_join(weekend_morning_od , busstop_hex,
            by = c("DESTIN_BS" = "BUS_STOP_N"))
```

```{r eval=FALSE}
duplicate <- weekend_morning_od %>%
  group_by_all() %>%
  filter(n()>1) %>%
  ungroup()
```

```{r eval=FALSE}
weekend_morning_od <- unique(weekend_morning_od)
```

```{r eval=FALSE}
weekend_morning_od <- weekend_morning_od %>%
  rename(DESTIN_HEX = hex_id) %>%
  drop_na() %>%
  group_by(ORIGIN_HEX, DESTIN_HEX) %>%
  summarise(WEEKEND_MORNING_PEAK = sum(TRIPS))
```

```{r eval=FALSE}
write_rds(weekend_morning_od, "data/rds/weekend_morning_od.rds")
```

```{r}
weekend_morning_od <- read_rds("data/rds/weekend_morning_od.rds")
```

### 5.4Weekend Evening Peak Flow

Repeat the process above for weekend/holiday evening peak (4-7 PM).

```{r eval=FALSE}
weekend_evening_od <- left_join(weekend16_19 , busstop_hex,
            by = c("ORIGIN_PT_CODE" = "BUS_STOP_N")) %>%
  rename(ORIGIN_BS = ORIGIN_PT_CODE,
         ORIGIN_HEX = hex_id,
         DESTIN_BS = DESTINATION_PT_CODE)
```

```{r eval=FALSE}
duplicate <- weekend_evening_od %>%
  group_by_all() %>%
  filter(n()>1) %>%
  ungroup()
```

```{r eval=FALSE}
weekend_evening_od <- unique(weekend_evening_od)
```

```{r eval=FALSE}
weekend_evening_od <- left_join(weekend_evening_od , busstop_hex,
            by = c("DESTIN_BS" = "BUS_STOP_N"))
```

```{r eval=FALSE}
duplicate <- weekend_evening_od %>%
  group_by_all() %>%
  filter(n()>1) %>%
  ungroup()
```

```{r eval=FALSE}
weekend_evening_od <- unique(weekend_evening_od)
```

```{r eval=FALSE}
weekend_evening_od <- weekend_evening_od %>%
  rename(DESTIN_HEX = hex_id) %>%
  drop_na() %>%
  group_by(ORIGIN_HEX, DESTIN_HEX) %>%
  summarise(WEEKEND_EVENING_PEAK = sum(TRIPS))
```

```{r eval=FALSE}
write_rds(weekend_evening_od, "data/rds/weekend_evening_od.rds")
```

```{r}
weekend_evening_od <- read_rds("data/rds/weekend_evening_od.rds")
```

Remove items no longer needed to free memory and avoid redundancy.

```{r}
rm(weekday6_9, weekday17_20, weekend11_14, weekend16_19)
```

## **6.Visualising Commute Flow**

### **6.1Removing Intra-zonal Flows**

The code filters out intra-zonal flows from four separate data frames by excluding rows where the origin hexagon (**ORIGIN_HEX**) is the same as the destination hexagon (**DESTIN_HEX**). This step is crucial for analysis as it removes trips that start and end within the same traffic analysis zone, focusing the study on inter-zonal movements which are more significant for understanding commuting patterns and the broader transportation network efficiency.

```{r}
weekday_morning_od1 <- weekday_morning_od[weekday_morning_od$ORIGIN_HEX!=weekday_morning_od$DESTIN_HEX,]
```

```{r}
weekday_afternoon_od1 <- weekday_afternoon_od[weekday_afternoon_od$ORIGIN_HEX!=weekday_afternoon_od$DESTIN_HEX,]
```

```{r}
weekend_morning_od1 <- weekend_morning_od[weekend_morning_od$ORIGIN_HEX!=weekend_morning_od$DESTIN_HEX,]
```

```{r}
weekend_evening_od1 <- weekend_evening_od[weekend_evening_od$ORIGIN_HEX!=weekend_evening_od$DESTIN_HEX,]
```

### 6.2Creating the Desire Lines

The code uses the `od2line` function to transform the inter-zonal flow data from each time period into **desire lines**, which are spatial representations of the volume and direction of trips between different hexagons. These lines are prepared for visualization, allowing us to graphically depict and analyze commuting patterns for different times of the day and week on a map.

```{r}
weekday_morning_flowLine <- od2line(flow = weekday_morning_od1, 
                    zones = hex_grid_sf,
                    zone_code = "hex_id")
```

```{r}
weekday_afternoon_flowLine <- od2line(flow = weekday_afternoon_od1, 
                    zones = hex_grid_sf,
                    zone_code = "hex_id")
```

```{r}
weekend_morning_flowLine <- od2line(flow = weekend_morning_od1, 
                    zones = hex_grid_sf,
                    zone_code = "hex_id")
```

```{r}
weekend_evening_flowLine <- od2line(flow = weekend_evening_od1, 
                    zones = hex_grid_sf,
                    zone_code = "hex_id")
```

### 6.3Visualising the Desire Lines

#### 6.3.1Weekday Morning Peak Commute Flow Map

This visualization code uses the **`tmap`** package to plot the hexagonal grid as a base, draws the boundaries of the metropolitan planning zones (MPZ), and overlays the **desire lines** representing high-volume weekday morning commute flows in Singapore. The lines are styled to vary in width and color intensity based on the volume of commutes, with thicker, darker lines indicating higher numbers of trips.

```{r}
tm_shape(hex_grid_sf) +
  tm_polygons(
    border.col = "grey50", 
    border.alpha = 0.6, 
    alpha = 0.1
  ) +
  tm_shape(weekday_morning_flowLine %>% 
             filter(WEEKDAY_MORNING_PEAK >= 5000)) +
  tm_lines(
    lwd = "WEEKDAY_MORNING_PEAK",
    style = "quantile",
    scale = c(0.1, 1, 3, 5, 7, 10),
    n = 6,
    alpha = 0.5,
    palette = "Blues"
  ) +
  tm_shape(mpsz) +
  tm_borders(
    col = "darkblue", 
    alpha = 0.1,
    lwd = 1.5
  ) +
  tm_layout(
    main.title = "Weekday Morning Commute Flows in Singapore",
    main.title.position = "center",
    main.title.size = 1.0,
    legend.title.size = 0.8,
    legend.text.size = 0.7,
    legend.position = c("left", "bottom"),
    frame = FALSE,
    inner.margins = c(0.05, 0.05, 0.05, 0.05)
  ) +
  tm_credits("Source: LTA DataMall", position = c("RIGHT", "BOTTOM"), size = 0.5)
```

[Insight from the Map:]{.underline}

The "Weekday Morning Commute Flows in Singapore" map reveals significant commuter traffic between various regions during peak morning hours. The thicker, darker lines suggest heavy flow between central business districts and outlying residential areas, indicating a typical urban commute pattern where many residents travel towards city centers for work. Areas with dense hexagon clusters, likely representing central and suburban residential zones, show extensive outward flow, highlighting these as key commuter hubs. Conversely, some regions exhibit sparse lines, suggesting lower population density or less reliance on public bus transit.

#### 6.3.2Weekday Afternoon Peak Commute Flow Map

```{r}
tm_shape(hex_grid_sf) +
  tm_polygons(
    border.col = "grey50", 
    border.alpha = 0.6, 
    alpha = 0.1
  ) +
  tm_shape(weekday_afternoon_flowLine %>% 
             filter(WEEKDAY_AFTERNOON_PEAK >= 5000)) +
  tm_lines(
    lwd = "WEEKDAY_AFTERNOON_PEAK",
    style = "quantile",
    scale = c(0.1, 1, 3, 5, 7, 10),
    n = 6,
    alpha = 0.5,
    palette = "Blues"
  ) +
  tm_shape(mpsz) +
  tm_borders(
    col = "darkblue", 
    alpha = 0.1,
    lwd = 1.5
  ) +
  tm_layout(
    main.title = "Weekday Afternoon Commute Flows in Singapore",
    main.title.position = "center",
    main.title.size = 1.0,
    legend.title.size = 0.8,
    legend.text.size = 0.7,
    legend.position = c("left", "bottom"),
    frame = FALSE,
    inner.margins = c(0.05, 0.05, 0.05, 0.05)
  ) +
  tm_credits("Source: LTA DataMall", position = c("RIGHT", "BOTTOM"), size = 0.5)
```

[Insight from the Map:\
]{.underline}The "Weekday Afternoon Commute Flows in Singapore" map suggests a reverse commute pattern from the morning, with significant flows from central areas to the outskirts, likely as people return home from work. The dense lines indicate high traffic volumes, particularly from the CBD and other employment hubs to residential districts. The distribution and volume of these lines can indicate areas with a high demand for evening public transportation services, and such insights could inform enhancements to bus service capacity and frequency to meet commuter needs during peak hours.

#### 6.3.3Weekend Morning Peak Commute Flow Map

```{r}
tm_shape(hex_grid_sf) +
  tm_polygons(
    border.col = "grey50", 
    border.alpha = 0.6, 
    alpha = 0.1
  ) +
  tm_shape(weekend_morning_flowLine %>% 
             filter(WEEKEND_MORNING_PEAK >= 3000)) +
  tm_lines(
    lwd = "WEEKEND_MORNING_PEAK",
    style = "quantile",
    scale = c(0.1, 1, 3, 5, 7, 13, 15),
    n = 7,
    alpha = 0.5,
    palette = "Blues"
  ) +
  tm_shape(mpsz) +
  tm_borders(
    col = "darkblue", 
    alpha = 0.1,
    lwd = 1.5
  ) +
  tm_layout(
    main.title = "Weekend Morning Commute Flows in Singapore",
    main.title.position = "center",
    main.title.size = 1.0,
    legend.title.size = 0.8,
    legend.text.size = 0.7,
    legend.position = c("left", "bottom"),
    frame = FALSE,
    inner.margins = c(0.05, 0.05, 0.05, 0.05)
  ) +
  tm_credits("Source: LTA DataMall", position = c("RIGHT", "BOTTOM"), size = 0.5)
```

[Insight from the Map:]{.underline}

The "Weekend Morning Commute Flows in Singapore" map shows a noticeable reduction in volume and fewer dense flow lines compared to weekdays, reflecting a typical decrease in commuting activity during weekends. The flows that are present may indicate travel to weekend-specific destinations like markets, recreational areas, or places of worship. The patterns suggest that the weekend movement is more dispersed and possibly oriented towards leisure or non-work-related activities, contrasting the concentrated, work-directed flows of weekday mornings.

#### 6.3.4Weekend Evening Peak Commute Flow Map

```{r}
tm_shape(hex_grid_sf) +
  tm_polygons(
    border.col = "grey50", 
    border.alpha = 0.6, 
    alpha = 0.1
  ) +
  tm_shape(weekend_evening_flowLine %>% 
             filter(WEEKEND_EVENING_PEAK >= 3000)) +
  tm_lines(
    lwd = "WEEKEND_EVENING_PEAK",
    style = "quantile",
    scale = c(0.1, 1, 3, 5, 7, 10),
    n = 6,
    alpha = 0.5,
    palette = "Blues"
  ) +
  tm_shape(mpsz) +
  tm_borders(
    col = "darkblue", 
    alpha = 0.1,
    lwd = 1.5
  ) +
  tm_layout(
    main.title = "Weekend Evening Commute Flows in Singapore",
    main.title.position = "center",
    main.title.size = 1.0,
    legend.title.size = 0.8,
    legend.text.size = 0.7,
    legend.position = c("left", "bottom"),
    frame = FALSE,
    inner.margins = c(0.05, 0.05, 0.05, 0.05)
  ) +
  tm_credits("Source: LTA DataMall", position = c("RIGHT", "BOTTOM"), size = 0.5)
```

[Insight from the Map:]{.underline}\
The "Weekend Evening Commute Flows in Singapore" map likely indicates an increase in volume compared to the morning, with more pronounced traffic flows towards residential areas and perhaps popular evening destinations. This contrasts with weekday evenings, where the flow would primarily be homeward from work centers. The patterns may suggest leisure and social activities are influencing travel, with possibly greater flows to entertainment or dining hubs and a more dispersed pattern as people return from various activities across the city.

```{r}
rm(duplicate, weekday_morning_flowLine, weekday_morning_od, weekday_morning_od1,
   weekend_morning_flowLine, weekend_morning_od, weekend_morning_od1,
   weekend_evening_flowLine, weekend_evening_od, weekend_evening_od1)
```

## 7.Attractiveness Factors

*In the subsequent section, we will delve into a targeted analysis of the weekday afternoon peak period. The reason for this focus stems from the comprehensive nature of commute drivers during this time. On weekday afternoons, residents are not only heading home from work or school but often transition to leisure activities such as shopping or entertainment venues directly. This complexity presents a rich tapestry of commuting patterns worthy of in-depth examination.*

### 7.1Entertainment Distribution Integration

Entertainment distribution is considered an attractiveness factor because these venues often draw people to travel to them, influencing commuting and traffic flows. The provided code reads in the **entertn** geospatial data and then calculates the count of entertainment venues within each hexagon of the **hex_grid_sf** grid, integrating this data to quantify the level of entertainment-related attractiveness of different areas.

```{r}
entertn <- st_read(dsn = "data/geospatial",
                      layer = "entertn")
```

```{r}
hex_grid_sf$entertn_count <- lengths(st_intersects(hex_grid_sf, entertn))
```

### 7.2Food & Beverage Distribution Integration

Food and beverage (F&B) venues act as attractiveness factors because they are destinations that people may frequently visit after work, thus influencing traffic and transportation patterns within an area.

```{r}
FB <- st_read(dsn = "data/geospatial",
                   layer = "F&B")
```

```{r}
hex_grid_sf$FB_count <- lengths(st_intersects(hex_grid_sf, FB))
```

### 7.3Leisure & Recreation Distribution Integration

Leisure and recreation spots are included in the study of weekday afternoon peak times because they are popular destinations that contribute to the flow of people and the overall demand for transportation services during these periods.

```{r}
lere <- st_read(dsn = "data/geospatial",
                   layer = "Liesure&Recreation")
```

```{r}
hex_grid_sf$lere_count <- lengths(st_intersects(hex_grid_sf, lere))
```

### 7.4Retail Distribution Integration

Retail locations are included in the analysis because they are key destinations that attract shoppers, impacting people's movement and transit patterns, especially during peak times.

```{r}
retail <- st_read(dsn = "data/geospatial",
                  layer = "Retails")
```

```{r}
hex_grid_sf$retail_count <- lengths(st_intersects(hex_grid_sf, retail))
```

### 7.5Train Exits Distribution Integration

```{r}
trainexits <- st_read(dsn = "data/geospatial",
                      layer = "Train_Station_Exit_Layer")
```

The code transforms the coordinates of train exit locations to match the coordinate reference system of the hexagonal grid and then counts the number of train exits within each hexagon. This reflects the influence of proximity to train stations on bus ridership, as people often use buses to access MRT stations in Singapore.

```{r}
trainexits <- st_transform(trainexits, st_crs(hex_grid_sf))
hex_grid_sf$trainexits_count <- lengths(st_intersects(hex_grid_sf, trainexits))
```

### 7.6Residence Distribution Integration

The code reads a CSV file containing housing data (**hdb.csv**), filters for entries marked as residential (**Y**), selects columns for latitude, longitude, and total dwelling units, and removes any rows with missing data. This prepares a dataset of residential locations and their capacity, which is important for analyzing homebound travel patterns.

```{r}
hdb <- read_csv("data/aspatial/hdb.csv")
```

```{r}
residential <- hdb %>%
  filter(residential == "Y") %>%
  select(lat, lng, total_dwelling_units) %>%
  na.omit()
```

This step converts the **residential** dataframe into a spatial dataframe with longitude and latitude coordinates, sets its coordinate reference system (CRS) to WGS 84 (CRS 4326), and then transforms it to match the CRS of the **hex_grid_sf** for compatibility in subsequent spatial merging.

```{r}
residential <- st_as_sf(residential, 
                           coords = c("lng", "lat"), 
                           crs = 4326) %>%
  st_transform(crs = st_crs(hex_grid_sf))
```

This step calculates the total number of dwelling units from the **residential** data within each hexagon of the **hex_grid_sf** grid. It uses spatial intersection to identify which residential entries fall into each hexagon and then sums the **total_dwelling_units** for those entries, assigning this sum to a new **residential_count** column in the hex grid.

```{r}
intersections <- st_intersects(hex_grid_sf, residential, sparse = TRUE)
hex_grid_sf$residential_count <- mapply(function(index, residential) {
  sum(residential$total_dwelling_units[index], na.rm = TRUE)
}, intersections, MoreArgs = list(residential = residential))
```

```{r}
rm(intersections)
```

## 8.Propulsive Factors

*To understand the propulsive factors behind bus commute flow, we focus on several key factors. Firstly, the density of bus stops, as captured by **bus_stop_count**, indicates the convenience and accessibility of bus services. Secondly, the distribution of businesses and schools sheds light on major daily locations, as these are places where people work and study, often traveling from these places during weekday afternoons. Additionally, we include the distribution of financial services, recognizing that these often coincide with business hubs and can be significant in influencing commuting patterns.*

### 8.1Business Distribution Integration

```{r}
business <- st_read(dsn = "data/geospatial",
                      layer = "Business")
```

```{r}
hex_grid_sf$business_count <- lengths(st_intersects(hex_grid_sf, business))
```

### 8.2School Distribution Integration

This code snippet queries an API to gather information about schools in Singapore. It first reads a CSV file listing school postal codes, then iterates through these codes, sending a request to the **OneMap** API for each. For postal codes with found data, it appends relevant details to the **found** dataframe. If no data is found for a postal code, it adds that code to the **not_found** dataframe.

```{r}
url<-"https://www.onemap.gov.sg/api/common/elastic/search"

csv<-read_csv("data/aspatial/Generalinformationofschools.csv")
postcodes<-csv$`postal_code`

found<-data.frame()
not_found<-data.frame()

for(postcode in postcodes){
  query<-list('searchVal'=postcode,'returnGeom'='Y','getAddrDetails'='Y','pageNum'='1')
  res<- GET(url,query=query)
  
  if((content(res)$found)!=0){
    found<-rbind(found,data.frame(content(res))[4:13])
  } else{
    not_found = data.frame(postcode)
  }
}
```

The next step merges the original school information from the **csv** file with the data retrieved from the API (**found**), matching entries based on postal codes. The merged data, which now includes enhanced details for each school, is saved to a new CSV file **schools.csv**. Additionally, postal codes for which no data was found are saved in a separate file **not_found.csv**, allowing for further review or data acquisition efforts.

```{r}
merged = merge(csv, found, by.x = 'postal_code', by.y = 'results.POSTAL', all = TRUE)
write.csv(merged, file = "data/aspatial/schools.csv")
write.csv(not_found, file = "data/aspatial/not_found.csv")
```

The code below initially reads the enhanced **schools.csv** file and renames latitude and longitude columns for clarity. It then selects specific columns, including postal code, school name, latitude, and longitude. Following this, it filters out any schools with missing latitude or longitude data. Finally, the filtered dataset is converted into a spatial dataframe with a coordinate reference system (CRS) of WGS 84 (CRS 4326) and then transformed to match Singapore's local CRS (3414) for spatial analysis compatibility.

```{r}
schools <- read_csv("data/aspatial/schools.csv") %>%
  rename(latitude = "results.LATITUDE",
         longitude = "results.LONGITUDE")%>%
  select(postal_code, school_name, latitude, longitude)
```

```{r}
schools <- schools %>%
  filter(!is.na(longitude) & !is.na(latitude)) %>%
  st_as_sf(coords = c("longitude", "latitude"), crs = 4326) %>%
  st_transform(crs = 3414)
```

```{r}
hex_grid_sf$school_count <- lengths(st_intersects(hex_grid_sf, schools))
```

```{r}
rm(csv, found, merged, not_found, query, res,
   postcode, postcodes, url)
```

### 8.3Financial Service Distribution Integration

```{r}
finserv <- st_read(dsn = "data/geospatial",
                      layer = "FinServ")
```

```{r}
hex_grid_sf$finserv_count <- lengths(st_intersects(hex_grid_sf, finserv))
```

### 8.4Final Check of Integrated Data

Before moving to next section, we may kindly check the basic structure and content of integrated data **hex_grid_sf**.

```{r}
datatable(hex_grid_sf)
```

## 9.Computing Distance Matrix

### **9.1Converting from sf Data.table to Spatial Polygons Data.frame**

This conversion is necessary for compatibility with spatial functions or methods that specifically require data in the **sp** format, rather than the **sf** format.

```{r}
hex_grid_sp <- as(hex_grid_sf, "Spatial")
hex_grid_sp
```

### **9.2 Computing the Distance Matrix**

The `spDists` function computes distances between each pair of hexagons in the grid. The parameter `longlat = FALSE` indicates that the distances are calculated in a planar manner, suitable for the coordinate system used. The final line displays the first 10 rows and columns of this distance matrix for a quick inspection of the calculated distances.

```{r}
dist <- spDists(hex_grid_sp, 
                longlat = FALSE)
head(dist, n=c(10, 10))
```

### **9.3Labeling Column and Row Headers of the Distance Matrix**

This code assigns the **hex_id** values from the **hex_grid_sf** dataframe as both column and row headers for the distance matrix **dist**. By doing so, each row and column in the matrix is clearly labeled with the corresponding hexagon's identifier, making it easier to interpret the distances between specific hexagons.

```{r}
hex_names <- hex_grid_sf$hex_id
```

```{r}
colnames(dist) <- paste0(hex_names)
rownames(dist) <- paste0(hex_names)
```

### **9.4Pivoting Distance Value by HEX_ID**

The code transforms the distance matrix **dist** into a long format data frame **distPair** using the `melt` function. Each row in **distPair** represents a pair of hexagons with their corresponding distance.

```{r}
distPair <- melt(dist) %>%
  rename(dist = value)
head(distPair, 10)
```

### **9.5Updating Intra-zonal Distances**

This code filters the **distPair** dataframe to exclude distances of zero (indicating the same hexagon), and then provides a statistical summary of the remaining non-zero distances.

```{r}
distPair %>%
  filter(dist > 0) %>%
  summary()
```

We then replace the distance of 0 by 200 which would not affect the minimum distance that is 433.

```{r}
distPair$dist <- ifelse(distPair$dist == 0,
                        200, distPair$dist)
```

Let's check the statistical summary of **distPair** again.

```{r}
distPair %>%
  summary()
```

Rename the **Var1** and **Var2** as **orig** and **dest** respectively which are short for origin and destination.

```{r}
distPair <- distPair %>%
  rename(orig = Var1,
         dest = Var2)
```

Export and save the **distPair** in rds format.

```{r}
write_rds(distPair, "data/rds/distPair.rds") 
```

## 10.Creating Complete Data for Spatial Interaction Modeling

This code converts the **ORIGIN_HEX** and **DESTIN_HEX** columns in the **weekday_afternoon_od1** dataframe, as well as the **orig** and **dest** columns in the **distPair** dataframe, into factors. This conversion is essential for spatial interaction modeling, as it ensures that these columns, representing hexagon identifiers, are treated as *categorical variables* rather than *numeric values*.

```{r}
weekday_afternoon_od1$ORIGIN_HEX <- as.factor(weekday_afternoon_od1$ORIGIN_HEX)
weekday_afternoon_od1$DESTIN_HEX <- as.factor(weekday_afternoon_od1$DESTIN_HEX)
distPair$orig <- as.factor(distPair$orig)
distPair$dest <- as.factor(distPair$dest)
```

The code below adds distance information to each origin-destination pair in the **weekday_afternoon_od1** data.

```{r}
weekday_afternoon_od2 <- weekday_afternoon_od1 %>%
  left_join (distPair,
             by = c("ORIGIN_HEX" = "orig",
                    "DESTIN_HEX" = "dest"))
```

The code converts the **hex_grid_sf** simple features (sf) object into a regular dataframe named **hex_grid_df**. It then selects specific columns related to various counts (like bus stops, businesses, schools, financial services, etc.) and the hexagon ID (**hex_id**).

```{r}
hex_grid_df <- as.data.frame(hex_grid_sf) %>%
  select(hex_id, bus_stop_count, business_count, school_count, finserv_count, 
         entertn_count, FB_count, lere_count, retail_count, trainexits_count, residential_count) %>%
  mutate(hex_id = as.character(hex_id))
```

**origin_factors** is created by selecting relevant columns from **hex_grid_df** that represent factors at the origin hexagons (like bus stop count, business count, etc.).

```{r}
origin_factors <- hex_grid_df %>%
  select(hex_id, bus_stop_count, business_count, school_count, finserv_count)
weekday_afternoon_od2 <- weekday_afternoon_od2 %>%
  mutate(ORIGIN_HEX = as.character(ORIGIN_HEX),
         DESTIN_HEX = as.character(DESTIN_HEX))
```

This join adds the origin-specific factors to each row based on the origin hexagon ID, enriching the dataset with contextual information about the origin locations.

```{r}
weekday_afternoon_od2_with_origin <- weekday_afternoon_od2 %>%
  left_join(origin_factors, by = c("ORIGIN_HEX" = "hex_id"))
```

**destin_factors** is created by selecting columns from **hex_grid_df** that represent factors at destination hexagons, such as counts of entertainment venues, food and beverage outlets, leisure and recreation spots, retail locations, train exits, and residential units.

```{r}
destin_factors <- hex_grid_df %>%
  select(hex_id, entertn_count, FB_count, lere_count, retail_count, trainexits_count, residential_count)
```

This step adds destination-specific contextual information to each row in the dataset, based on the destination hexagon ID, thus completing the dataset with both origin and destination factors

```{r}
weekday_afternoon_od2_complete <- weekday_afternoon_od2_with_origin %>%
  left_join(destin_factors, by = c("DESTIN_HEX" = "hex_id"))
```

```{r}
glimpse(weekday_afternoon_od2_complete)
```

Export the complete dataset and name it as SIM_data for later use.

```{r}
write_rds(weekday_afternoon_od2_complete, "data/rds/SIM_data.rds")
```

```{r}
rm(business, busstop, busstop_hex, busstop_mpsz, destin_factors, dist, distPair, entertn, FB, finserv, hdb, hex_grid_df, hex_grid_sf, hex_grid_sp, lere, mpsz, origin_factors, residential, retail, schools, trainexits, weekday_afternoon_flowLine, weekday_afternoon_od, weekday_afternoon_od1, weekday_afternoon_od2, weekday_afternoon_od2_complete, weekday_afternoon_od2_with_origin, hex_names)
```

## **11.Calibrating Spatial Interaction Models**

### **11.1Importing the Modelling Data**

The first step to calibrate Spatial Interaction Models is importing the modelling data by using the code chunk below.

```{r}
SIM_data <- read_rds("data/rds/SIM_data.rds")
```

### **11.2Visualising the Dependent Variable**

After plotting the distribution of dependent variable, the bus travel volume during weekday afternoon peak, it's obvious that the distribution is highly skewed and not resemble bell shape or also known as normal distribution.

```{r}
ggplot(data = SIM_data, aes(x = WEEKDAY_AFTERNOON_PEAK)) +
  geom_histogram(binwidth = 100) +
  xlab("Volume of Weekday Afternoon Peak") +
  ylab("Count") +
  ggtitle("Distribution of Weekday Afternoon Peak Volumes")

```

Next, let us visualise the relation between the dependent variable and one of the key independent variable in Spatial Interaction Model, namely distance.

```{r}
ggplot(data = SIM_data, aes(x = dist, y = WEEKDAY_AFTERNOON_PEAK)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", se = FALSE) +
  xlab("Distance Between Hexagons") +
  ylab("Volume of Weekday Afternoon Peak") +
  ggtitle("Relationship Between Distance and Weekday Afternoon Peak Volume")
```

Notice that their relationship hardly resemble linear relationship.

On the other hand, if we plot the scatter plot by using the log transformed version of both variables, we can see that their relationship is more resemble linear relationship.

```{r}
ggplot(data = SIM_data, aes(x = log(dist), y = log(WEEKDAY_AFTERNOON_PEAK))) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", se = FALSE, color = "lightblue") +
  xlab("Log of Distance Between Hexagons") +
  ylab("Log of Volume of Weekday Afternoon Peak") +
  ggtitle("Log-Log Relationship Between Distance and Volume of Weekday Afternoon Peak") +
  theme_minimal()
```

### **11.3Checking and Replacing Variables with Zero Values**

Since Poisson Regression is based of log and log 0 is undefined, it is important for us to ensure that no 0 values in the explanatory variables.

In the code chunk below, `summary()` is used to compute the summary statistics of all variables in **SIM_data** data frame.

```{r}
summary(SIM_data)
```

After knowing variables with 0 values, we should use the code below to replace all 0 values to 0.99.

```{r}
SIM_data$business_count <- ifelse(
  SIM_data$business_count == 0,
  0.99, SIM_data$business_count)
SIM_data$school_count <- ifelse(
  SIM_data$school_count == 0,
  0.99, SIM_data$school_count)
SIM_data$finserv_count <- ifelse(
  SIM_data$finserv_count == 0,
  0.99, SIM_data$finserv_count)
SIM_data$entertn_count <- ifelse(
  SIM_data$entertn_count == 0,
  0.99, SIM_data$entertn_count)
SIM_data$FB_count <- ifelse(
  SIM_data$FB_count == 0,
  0.99, SIM_data$FB_count)
SIM_data$lere_count <- ifelse(
  SIM_data$lere_count == 0,
  0.99, SIM_data$lere_count)
SIM_data$retail_count <- ifelse(
  SIM_data$retail_count == 0,
  0.99, SIM_data$retail_count)
SIM_data$trainexits_count <- ifelse(
  SIM_data$trainexits_count == 0,
  0.99, SIM_data$trainexits_count)
SIM_data$residential_count <- ifelse(
  SIM_data$residential_count == 0,
  0.99, SIM_data$residential_count)
```

Let's run `summary()` again for double check.

```{r}
summary(SIM_data)
```

### **11.4Unconstrained Spatial Interaction Model**

![Figure 1. Definition of Unconstrained Model](images/unc.png)

The model is "unconstrained" because it doesn't impose any limitations on the total outflow from an origin or the total inflow to a destination, allowing them to vary freely based on the underlying variables.

The code chunk used to calibrate to model is shown as below. Specifically, we should export and save uncSIM to ensure reproducibility as it commonly takes long to run for large dataset.

```{r eval=FALSE}
uncSIM <- glm(formula = WEEKDAY_AFTERNOON_PEAK ~ 
                log(bus_stop_count) + 
                log(business_count) +
                log(school_count) +
                log(finserv_count) +
                log(entertn_count) +
                log(FB_count) +
                log(lere_count) +
                log(retail_count) +
                log(trainexits_count) +
                log(residential_count) +
                log(dist),
              family = poisson(link = "log"),
              data = SIM_data,
              na.action = na.exclude)
write_rds(uncSIM, "data/rds/uncSIM.rds")
```

```{r}
uncSIM <- read_rds("data/rds/uncSIM.rds")
uncSIM
```

**11.5R-squared Function\
**The provided R function `CalcRSquared` calculates the coefficient of determination, commonly known as R-squared (R²), to assess the goodness of fit for a statistical model.

Inside the function, it computes the correlation coefficient **r** between the observed and estimated values, then squares this value to obtain R².

```{r}
CalcRSquared <- function(observed,estimated){
  r <- cor(observed,estimated)
  R2 <- r^2
  R2
}
```

Next, we will compute the R-squared of the unconstrained SIM by using the code chunk below.

```{r}
CalcRSquared(uncSIM$data$WEEKDAY_AFTERNOON_PEAK, uncSIM$fitted.values)
```

The function `r2_mcfadden` calculates McFadden's R-squared for the provided **uncSIM** model, which is the unconstrained model in this context. McFadden's R-squared is a measure of goodness of fit for logistic regression models and is interpreted as *the proportion of the log likelihood* explained by the model. Unlike the traditional R-squared, McFadden's R-squared typically provides smaller values; values between 0.2 to 0.4 can indicate a good fit.

```{r}
r2_mcfadden(uncSIM)
```

### 11.6**Origin Constrained Spatial Interaction Model**

![Figure 2. Definition of Origin Constrained Model](images/orc-01.png)

The origin (production) constrained model predicts flows from origins to various destinations while ensuring that the sum of the flows from each origin matches the observed total outflows for that origin.

The code chunk below is adjusted accordingly:

```{r eval=FALSE}
orcSIM <- glm(formula = WEEKDAY_AFTERNOON_PEAK ~
                ORIGIN_HEX +
                log(entertn_count) +
                log(FB_count) +
                log(lere_count) +
                log(retail_count) +
                log(trainexits_count) +
                log(residential_count) +
                log(dist),
              family = poisson(link = "log"),
              data = SIM_data,
              na.action = na.exclude)
write_rds(orcSIM, "data/rds/orcSIM.rds")
```

```{r}
orcSIM <- read_rds("data/rds/orcSIM.rds")
orcSIM
```

```{r}
CalcRSquared(orcSIM$data$WEEKDAY_AFTERNOON_PEAK, orcSIM$fitted.values)
```

```{r}
r2_mcfadden(orcSIM)
```

### **11.7Destination Constrained Spatial Interaction Model**

![Figure 3. Definition of Destination Constrained Model](images/dec.png)

The destination (attraction) constrained model predicts flows to destinations by applying a balancing factor to ensure that the total predicted inflows to each destination match the observed inflows. Unlike the origin constrained model, this model accounts for the capacity or attractiveness of each destination to attract inflows.

The code chunk is adjusted accordingly as below:

```{r eval=FALSE}
decSIM <- glm(formula = WEEKDAY_AFTERNOON_PEAK ~
                DESTIN_HEX +
                log(bus_stop_count) + 
                log(business_count) +
                log(school_count) +
                log(finserv_count) +
                log(dist),
              family = poisson(link = "log"),
              data = SIM_data,
              na.action = na.exclude)
write_rds(decSIM, "data/rds/decSIM.rds")
```

```{r}
decSIM <- read_rds("data/rds/decSIM.rds")
decSIM
```

```{r}
CalcRSquared(decSIM$data$WEEKDAY_AFTERNOON_PEAK, decSIM$fitted.values)
```

```{r}
r2_mcfadden(decSIM)
```

### 11.8Doubly Constrained Spatial Interaction Model

![Figure 4. Definition of Doubly Constrained Model](images/dbc-02.png)

The doubly constrained model ensures that the predicted flows both from origins (*Oi*) and to destinations (*Dj*) match observed totals by applying balancing factors (*Ai* for origins and *Bj* for destinations) along with the influence of distance (*dij*).

The code chunk is changed correspondingly as below:

```{r eval=FALSE}
dbcSIM <- glm(formula = WEEKDAY_AFTERNOON_PEAK ~ 
                ORIGIN_HEX + 
                DESTIN_HEX + 
                log(dist),
              family = poisson(link = "log"),
              data = SIM_data,
              na.action = na.exclude)
write_rds(dbcSIM, "data/rds/dbcSIM.rds")
```

```{r}
dbcSIM <- read_rds("data/rds/dbcSIM.rds")
dbcSIM
```

```{r}
CalcRSquared(dbcSIM$data$WEEKDAY_AFTERNOON_PEAK, dbcSIM$fitted.values)
```

```{r}
r2_mcfadden(dbcSIM)
```

### 11.9Model Comparison

Create a list called **model_list** by using the code chunk below.

```{r}
model_list <- list(unconstrained=uncSIM,
                   originConstrained=orcSIM,
                   destinationConstrained=decSIM,
                   doublyConstrained=dbcSIM)
```

Next, we will compute the RMSE of all the models in **model_list** file by using the code chunk below.

```{r}
compare_performance(model_list,
                    metrics = "RMSE")
```

The print above reveals that doubly constrained SIM is the best model among all the four SIMs because it has the smallest RMSE value of 576.958.

### 11.11Visualizing Fitted

We are going to visualize the observed values and the fitted values of all models in this section. Firstly, we need to extract the fitted values from each model by using the code below.

```{r}
df <- as.data.frame(uncSIM$fitted.values) %>%
  round(digits = 0)
```

Next, we will join the fitted values to **SIM_data** data frame and name them appropriately.

```{r}
SIM_data <- SIM_data %>%
  cbind(df) %>%
  rename(uncWEEKDAY_AFTERNOON_PEAK = "uncSIM$fitted.values")
```

Repeat the process for orcSIM.

```{r}
df <- as.data.frame(orcSIM$fitted.values) %>%
  round(digits = 0)
```

```{r}
SIM_data <- SIM_data %>%
  cbind(df) %>%
  rename(orcWEEKDAY_AFTERNOON_PEAK = "orcSIM$fitted.values")
```

Repeat the process for decSIM.

```{r}
df <- as.data.frame(decSIM$fitted.values) %>%
  round(digits = 0)
```

```{r}
SIM_data <- SIM_data %>%
  cbind(df) %>%
  rename(decWEEKDAY_AFTERNOON_PEAK = "decSIM$fitted.values")
```

Repeat the process for dbcSIM.

```{r}
df <- as.data.frame(dbcSIM$fitted.values) %>%
  round(digits = 0)
```

```{r}
SIM_data <- SIM_data %>%
  cbind(df) %>%
  rename(dbcWEEKDAY_AFTERNOON_PEAK = "dbcSIM$fitted.values")
```

```{r}
unc_p <- ggplot(data = SIM_data,
                aes(x = uncWEEKDAY_AFTERNOON_PEAK,
                    y = WEEKDAY_AFTERNOON_PEAK)) +
  geom_point() +
  geom_smooth(method = lm)

orc_p <- ggplot(data = SIM_data,
                aes(x = orcWEEKDAY_AFTERNOON_PEAK,
                    y = WEEKDAY_AFTERNOON_PEAK)) +
  geom_point() +
  geom_smooth(method = lm)

dec_p <- ggplot(data = SIM_data,
                aes(x = decWEEKDAY_AFTERNOON_PEAK,
                    y = WEEKDAY_AFTERNOON_PEAK)) +
  geom_point() +
  geom_smooth(method = lm)

dbc_p <- ggplot(data = SIM_data,
                aes(x = dbcWEEKDAY_AFTERNOON_PEAK,
                    y = WEEKDAY_AFTERNOON_PEAK)) +
  geom_point() +
  geom_smooth(method = lm)

ggarrange(unc_p, orc_p, dec_p, dbc_p,
          ncol = 2,
          nrow = 2)
```

The trend line of **dbcSIM** indicates a stronger relationship with a steeper slope, suggesting this model may be capturing the variance in the observed data more effectively.
